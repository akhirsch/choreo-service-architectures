%!TEX root = desc.tex

%thrust1
\subsection{Thrust 1: Polymorphism-Enabled Features}
\label{sec:t1}

The first thrust of this work will design new features for choreographic languages
that rely on first-class process polymorphism and
are needed to substantially broaden the range of possible choreographic applications.

\subsubsection{Spawning New Locations}
\label{sec:spawning}

The first feature is the ability to spawn and kill new locations---computational units representing threads, processes, nodes, etc---% DO NOT DELETE, COMMENT IS IMPORTANT FOR SPACING
a core feature of many concurrent systems.
A few early choreographic works recognized its importance~\citep{CarboneM13,CruzFilipeM16a},
but those works lacked critical features for realistic programming, like higher-order operations,
and required locations to be spawned only to execute specific pre-defined operations.

We propose a much more general structure, where there is little distinction between an existing location and a newly-spawned one,
and threads may be killed when they are no longer needed.
Moreover, we will integrate this dynamic location management with a host of modern choreographic advances,
including higher-order functions and process polymorphism.

\begin{example}{Dynamic Thread Pool}
Many systems incorporate thread pools that dynamically resize based on demand.
That is, at high-demand times when requests are regularly waiting on threads, they spawn more,
and at low-demand times when threads are regularly sitting idle, excess threads are killed.
The proposed work would allow us to implement such thread pools as choreographies
with code akin to the following, where~$F$ is a local thunk provided by the client~$\Client$
specifying the computation to run.
\[
  \addtocounter{numlevels}{1}
  \Fun{\programfont{runJob}}{F}{
%    \LetInRaw*{
%      \begin{array}[t]{@{}l@{}}
%        T \ChorDef \ITE*{\Mngr.\programfont{enoughWorkers()}}
%                        {(\Mngr.\programfont{selectWorker}() \ColSend \Client)}
%                        {(\Mngr.\ChorFont{spawn} \ColSend \Client)} \\
%        T.f \ChorDef F \ColSend T \\
%        \Client.\mathit{res} \ChorDef T.(f~()) \ColSend \Client
%      \end{array}
%    }{
%      \Client.\text{``}\programfont{done}\text{''} \ColSend \Mngr \seq
%      \Mngr.\programfont{releaseOrKillWorker}(T) \seq
%      \Client.\mathit{res}
%    }
    \LetMany{{T}{\ITE*{\Mngr.\programfont{enoughWorkers()}}{\Mngr.\programfont{selectWorker}() \ColSend \Client}{\Mngr.\ChorFont{spawn} \ColSend \Client}}%
             {T.f}{F \ColSend T}%
             {\Client.\mathit{res}}{T.(f~()) \ColSend \Client}}{%
      \Client.\text{``}\programfont{done}\text{''} \ColSend \Mngr \seq
      \Mngr.\programfont{releaseOrKillWorker}(T) \seq
      \Client.\mathit{res}
    }
  }
  \addtocounter{numlevels}{-1}
\]

The PIs recent work on first-class location polymorphism showed how to
implement \emph{static} thread pools with a fixed set of threads~\citep{SamuelsonHC25}.
However, that work assumes the set of threads in the entire system is fixed,
inherently preventing dynamically resizing the thread pool.
\end{example}

Safely spawning and killing locations in a choreography---be they threads, processes, or entire nodes---poses three distinct problems,
all illustrated by the above example.
\begin{enumerate}
  \item\label{spawn:li:name}
    Existing locations must be made aware of the new one in a way that allows them to interact with it.
    Otherwise the new location cannot send or receive messages, meaning they cannot participate in the choreography in any meaningful way.

    \textit{Example:} Clients of newly-spawned threads must know where to send their jobs.

  \item\label{spawn:li:kill}
    If a location is killed, living locations must know not to send to or receive from the terminated location.
    A retained reference to a dead name can result in a live location attempting to contact a dead one, producing deadlock.

    \textit{Example:} The pool manager and clients must properly discard all references to a killed thread.

  \item\label{spawn:li:code}
    The new location must know what code to execute, and it must match the code existing nodes expect.
    Without the correct code, the new location and existing locations might not have matching message sends and receives, causing deadlock.

    \textit{Example:} New threads must know to wait for instructions from a client and execute them
    and reply with the result, or the client will deadlock.
\end{enumerate}

Problem~\ref{spawn:li:name} is immediately solved by first-class location polymorphism.
Upon spawning a new thread, we give it a new name and bind that name to a location variable.
That location now functions just as any other polymorphic location variable;
it can be transmitted across the network or used as an endpoint for sending and receiving messages.
Problems~\ref{spawn:li:kill} and~\ref{spawn:li:code}---both of which can cause deadlock if not properly addressed---require additional research.

A key insight is that Problem~\ref{spawn:li:kill} closely resembles classic memory management concerns.
Just as memory should not be referenced after being freed, lest the application crash or expose critical security vulnerabilities,
locations should not be referenced after they are killed, lest the application deadlock.
Similarly, if a location's computation has completed, it should be killed to avoid leaking resources,
just memory locations should be freed when their values are no longer needed.

There are, however, two important differences between our setting and traditional memory management.
Making our setting simpler is that, unlike values in memory which can themselves contain memory addresses,
location names are opaque and it is the top-level code that refers to them.
We therefore have no need for the sort of reachability analysis employed by many automatic memory management systems.
Indeed, simple reference counting would be sufficient to determine when no more code references a spawned location.
Making our setting considerably more challenging, however, is Problem~\ref{spawn:li:code} mentioned above:
new locations must know what code to execute.
With an entirely dynamic lifetime, it is not clear what code a new location will need or how to determine that when spawning it.

We propose to investigate how various automatic memory management strategies translate to the context spawned choreographic location management.
In particular, we have substantial preliminary results showing the plausibility of scope-based lifetimes.
We will also investigate other techniques, including reference counting---simplified in our setting---and Rust-style linearity
to control aliasing and more directly identify inaccessible locations.

\begin{goal}
  \label{goal:spawn}
  Design and implement dynamic location spawning and killing in choreographies while maintaining deadlock-freedom guarantees.
\end{goal}

\paragraph{Preliminary Work: Scoped Locations}
Preliminary work by both PIs and PI Cecchetti's graduate student
has shown that basing the lifetime of a spawned location on a syntactic scope in the choreography is a viable approach.
In particular, syntactic scoping provides a simple solution to Problem~\ref{spawn:li:code}:
the code for a spawned thread~$T$ is simply the $T$-projection of the code for which it is in scope.

In the thread pool example, if there are too many requests, the pool manager could easily spawn a new thread
to process an individual request and that thread would automatically die after processing is complete.
The code for this case might look as follows.
As above, $F$~is a local thunk specifying~\Client's computation.
\[
  \addtocounter{numlevels}{1}
  \ForkIn{\Mngr}{T \ColSend \Client}{%
    \LetMany{{T.f}{F \ColSend W}{\Client.\mathit{res}}{T.(f~()) \ColSend \Client}}
            {\Client.\mathit{res}}
  }
  \addtocounter{numlevels}{-1}
\]
This code specifies that the pool manager~$\Mngr$ forks a new thread,
binds the name of that thread to~$T$, and sends it to the client~$\Client$.
The client then sends its job to the thread who runs it and passes the result back to the client.

Just as scoped memory management must ensure that references do not escape their scope through aliases or closures,
we also need to ensure references to~$T$ remain confined.
%Just as alias analysis is important to scope-based memory management,
%we also need to ensure references to~$T$ do not escape the scope in which the thread is alive.
%Ensuring that~$T$ is only referenced while it is alive is more complicated.
%While the scoping provides a syntactic boundary for when such a reference is safe,
%it does not automatically guarantee that no thread can attempt to reference~$T$ after it dies.
Consider the following choreography that forks a process and returns a function closing over its name: $\ForkIn{\Alice}{T}{\LamN \_\ldotp \Alice.42 \ColSend T}$.
This function, if applied, would immediately cause deadlock, as~\Alice attempts to send~$42$ to a thread that is now dead.

Our preliminary work indicates that we can resolve this dilemma by tracking the locations involved in a function
and verifying that any function returned from a~\ForkN block does not reference the spawned location.
Specifically, augmenting function types with the set of locations needed to compute the function
allows the typing rule for~\ForkN to verify that those locations are all in scope---and thus alive---outside the~\ForkN block.
Since location name variables are immutable, aliasing a name to a variable in a larger scope is not possible,
so addressing the closure concern in this way solves the problem.

\paragraph{Proposed Work: Spawned Locations with Unbounded Lifetimes}
Our scope-based preliminary work provides a clean way to identify what code to provide a new location
and makes clear that \emph{some} form of location spawning is viable, it is not without drawbacks.
In particular, it is unable to handle new locations with indefinite lifetimes.
For example, the thread pool may not wish to spawn additional threads only for a single task,
but instead simply increase the pool capacity when demand is high
and be willing to kill any thread that happens to be idle when demand is low.

To support this more flexible mode of dynamic locations, we will investigate how other memory management techniques
transfer into the choreographic location management domain.
We will especially focus on how to provide the correct code to new locations with no static bound on their lifetime.
Two promising approaches are (1)~to give new parties a view of the entire application code,
and (2)~restrict new parties to specific behavior fixed when they are spawned.
The first option would require existing locations to keep extra copies of the entire application code if they are able to initiate a spawn.
The second would need static verification that existing locations only attempt to interact with a new party
in accordance with the code it is provided, which would require statically tracking what different locations are able to do.

\iffalse
% This is a bunch of stuff I wrote before we realized it was all memory management.
% I'm leaving it for now because it was never in git before

\paragraph{Proposed Work: Linear Locations with Fractional Permissions}
The scoped-based process forking described above allows for any application that spawns new locations to handle individual tasks.
While many such applications exist---making it a powerful addition to the choreographic toolbox---% DO NOT DELETE, COMMENT IS IMPORTANT FOR SPACING
some systems require dynamic locations with indefinite lifetimes.
For example, the thread pool may not wish to spawn additional threads only for a single task,
but instead simply increase the pool capacity when demand is high
and be willing to kill any thread that happens to be idle when demand is low.

To support this more flexible mode of dynamic locations, we propose a second, complementary approach to
ensuring terminated threads are not referenced---and thus do not cause deadlock.
Specifically, we propose tracking locations linearly in a type system.

Linear types~\todo{cite} are a way of ensuring that values are used exactly once.
They are common when tracking resources, such as money, that cannot be safely created or destroyed~\citep{move-lang,CoblenzOE+20,DasBHPS21}.
Operations using linear values consume them, but may create a new reference if they are still usable afterward.

We propose to apply this idea to spawned location in choreographies,
allows us to correctly track which locations exist without confining them to a syntactic program scope.
In particular, if location names are linear, then only one reference to each location can exist at a time.
When a location is involved in a normal operation after which it continues executing---message sending or local computation---% DO NOT DELETE, COMMENT IS IMPORTANT FOR SPACING
the choreography can simply produce another reference to the same location, indicating that it can be used again.
When a location is killed, however, such as a thread pool manager culling unneeded threads,
that instruction consumes the only reference to the location, correctly rendering inaccessible to the rest of the program.

This approach allows us to implement the dynamic thread pool described above.
Upon spawning a thread, \Mngr~adds the linear reference to that thread to the pool.
When assigning a job, \Mngr~passes the linear reference to~\Client, removing it from the pool.
After a job is complete, \Client~returns the linear reference to~\Mngr, ensuring that they will not reference the thread again,
and~\Mngr can either place it back in the pool or kill it.

\ethan{
  Interesting point I want to make, but I'm not sure how:
  linearity is not uncommon in concurrent settings to prevent things like race conditions.
  In those contexts, the object belongs to one thread at a time, and they can pass it around (basically just ownership).
  That's \emph{not} what we need here.
  We don't care if multiple locations are talking to this one linear location at a time,
  we care that we can track what part of the \emph{code} the location exists in
  so that we know we can't kill it and still have it exist somewhere else.
  It's essentially just an alias analysis.
}

\paragraph{Fractional Permissions}
One severe limitation of the linear locations described above is that a location can only exist in one context at a time.
That restriction prevents (non-linear) functions from closing over location names
or from multiple independent parties from concurrently 

Specifically, we propose tracking fractional resources using a linear type system.

Fractional resource tracking dates back over 20 years~\citep{Boyland03,BornatCOP05}

\ethan{Things we need to discuss:
\begin{outline}{}
  \item What is fractional permission tracking?
  \item How is it used?
    \begin{lvl}
      \item Referencing location needs non-zero permission
      \item Killing location consumes full permission
      \item Connects to scoped locations by tracking permissions used in function types
    \end{lvl}
  \item How do we know what code to give a new location?
    \begin{lvl}
      \item This is a core research question of this sub-thrust
      \item Might tie into wait-until-called stuff in Thrust 3?
        If we give it wait-until-called code, it's obvious what it needs to do and how to use it.
    \end{lvl}
\end{outline}}

%\ethan{There are multiple problems to consider here.
%\begin{enumerate}[nosep]
%  \item How does the new location know what code to execute?
%  \item If you kill a location, how do you make sure you don't reference it again?
%    This comes in two different forms: regular instructions, and killing it twice.
%\end{enumerate}
%We propose two ways of handling this: scoping and linearity.
%We have preliminary work showing how to make scoping work, but it's missing some important features (e.g., thread pools).
%We will finish that and figure out to make linearity work.}
\fi

\subsubsection{Message Multi-Receives}
\label{sec:multi-receive}

The second new feature is a generalization of receiving a message that allows one recipient to receive multiple messages.
Existing choreographies allow a single location to \emph{send} messages to multiple recipients in a single (choreographic) instruction~\citep{BatesK+25,SamuelsonHC25} (a broadcast),
but no choreographies allow a single location to \emph{receive} messages from multiple senders as a single operation.

The power of this primitive becomes apparent when we require only a subset of the messages to arrive before the recipient can continue.
This is a fundamental primitive currently missing from the choreographic literature that is necessary to build many systems,
including distributed consensus protocols like Paxos~\citep{Lamport98}.
In such protocols, each node waits for votes from all others on what operation to perform next,
but must be able to continue once messages from half of the other nodes have arrived.

To accomplish this goal, we will add a new syntactic form to choreographies:
\[
  \{\ell_1.e_1, \dotsc, \ell_n.e_n\} \multircv{k} \ell
\]
This term indicates that locations $\ell_1, \dotsc, \ell_n$ should run computations~$e_1, \dotsc, e_n$, respectively, and send the resulting values to~$\ell$.
Location~$\ell$ can then accept those messages in any order and should continue executing once~$k$ of them arrive.
Instead of getting a single value, as in normal message passing, $\ell$~will receive a list of~$k$ values, one for each message.

Making this feature useful fundamentally relies on process polymorphism.
When~$\ell$ receives its list of~$k$ messages, it must be able to identify who sent those messages
to be able to respond or even simply record which ones arrived.
With first-class process polymorphism, identifying the sender is simple;
the message list from the multi-receive contains not just the value of each message, but a sender--value pair for each.

\paragraph{Message-Based Triggered Events}
Some applications---like Byzantine fault tolerant~(BFT) consensus protocols---wait not just for a fixed number of messages,
but for messages that satisfy a more complicated condition---like enough messages \emph{with the same value}.
The above-proposed multi-receive is powerful, but is not expressive enough to implement this functionality.
We propose to further empower these multi-receive operations by extending them to support \emph{triggered events}.

A triggered event consists of two components:
a predicate over a message list that determines when the event triggers,
and a computation to run when the predicate is satisfied.
Triggered events will be specified using the following switch-like syntax.
\[
  \{\ell_1.e_1, \dotsc, \ell_n.e_n\} \ColSend \ell ~\ChorFont{for~triggers}~ (P_1 \Rightarrow C_1) \dotsb (P_k \Rightarrow C_k)
\]
Here $P_1, \dotsc, P_k$ are the predicates and $C_1, \dotsc, C_k$ are choreographies
specifying which computation to perform when the corresponding event triggers.
Note that these are full choreographies, meaning they can involve computation by locations other than~$\ell$,
but only~$\ell$ inherently knows which event triggered.
Much like choreographic conditional statements, this creates a knowledge of choice problem~\ethan{cite?}.
We will resolve this challenge using a generalization of the selection messages present in existing work~\citep[see, e.g.,][]{Montesi13,HirschG22,Montesi23,GraversenHM24}
that selects between an arbitrary set of triggers, rather than only a two-way conditional branch.

Note that trigger events are a strict generalization of the $k$-of-$n$ multi-receives proposed above.
We can implement $\{\ell_1.e_1, \dotsc, \ell_n.e_n\} \multircv{k} \ell$ using triggers as
\[
  \{\ell_1.e_1, \dotsc, \ell_n.e_n\} \ColSend \ell ~\ChorFont{for~triggers}~ ((\programfont{length}~\mathit{msgs}) \geq k \Rightarrow \ell.\mathit{msgs}).
\]
However, the triggered events are considerably more expressive.
A node in a BFT consensus protocol with the~$3f + 1$ nodes---the minimum to tolerate~$f$ failures---might trigger a commit
when it receives~$f + 1$ votes for the same value.
With dishonest nodes, that could occur as soon as $f + 1$ messages arrive, or as late as $2f + 1$ messages.
Triggered events will thus provide developers a means to specify these conditions and allow nodes to proceed immediately when they are satisfied.

We will explore several different aspects of triggered events.
First, we will investigate how to specify the predicates.
Predicates should be able to examine both location names and values that have arrived,
making the local language of the choreography and obvious choice.
However, it is not clear how the system should behave if a trigger predicate fails to terminate.
We will therefore investigate if there are appropriate terminating languages for specifying predicates.

A second question arises when predicates are not mutually exclusive.
What should happen if multiple predicates are satisfied?
The choreography could execute only the first event to trigger,
and either choose nondeterministically if multiple triggers are satisfied simultaneously
or require the choreography to specify a priority order.
It could also execute all triggered events in some order.
We will investigate which of these options is most appropriate,
both by assessing their technical complexity and by surveying a variety of concurrent applications to determine their value to developers.

\begin{goal}
  \label{goal:multi-receive}
  Design and implement a multi-receive feature for choreographies
  where the recipient can proceed before all messages have arrived,
  including by defining custom triggered events.
\end{goal}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "desc.tex"
%%% End:
