%thrust2
\subsection{Thrust 2: Improved Compilation for Process Polymorphism}
\label{sec:t2}

\iffalse
\begin{outline}{Thrust 2 Outline}
\item Current status of Process Polymorphism
  \begin{lvl}
  \item Trick in EPP: Autognosis
    \begin{lvl}
    \item Every process knows its own identity
    \end{lvl}
  \item Formalized as $\AmIinN$
    \begin{lvl}
    \item Branches depending on whether the current process is in some set
    \end{lvl}
  \item Current state of the art: \emph{every} process-polymorphic function introduces $\AmIinN$ everywhere.
  \end{lvl}
\item Critique
  \begin{lvl}
  \item Every process-polymorphic function has its code doubled
    \begin{lvl}
    \item Can lead to expondential blowup of code size
    \item Linear cost to performance
    \end{lvl}
  \item This seems to be inherent for worst-case
    \begin{lvl}
    \item Random paths example
    \end{lvl}
  \item Often, it is unnecessary
  \end{lvl}
\item Goal: Cheap Process Polymorphism
  \begin{lvl}
  \item Static analyis
    \begin{lvl}
    \item Must
    \item Might
    \end{lvl}
  \item Switch-case $\programfont{AmI}\mathord{\in}\programfont{?}$
    \begin{lvl}
    \item Make statically-known disjoint sets flat
    \item Do they need to be disjoint?
      \begin{lvl}
      \item First-match semantics
      \end{lvl}
    \end{lvl}
  \end{lvl}
\item Experimental Verification of Efficiency Gains
  \begin{lvl}
  \item Goal: Develop benchmarks for process polymorphic EPP
    \begin{lvl}
    \item Contains worst-case example
    \item Contains examples that our techniques can optimize
    \item Contains ``representative'' examples
    \end{lvl}
  \item Goal: Build new EPP into compiler
    \begin{lvl}
    \item Undergraduate training
    \item Prior Work
    \end{lvl}
  \item Results of our benchmark
    \begin{lvl}
    \item Goal: Experimentally verify our optimizations are useful in practice.
    \end{lvl}
  \end{lvl}
\end{outline}
\fi

Improving compilation requires both designing improved techniques and validating the improvements.
Compilation of process-polymorphic choreographies lacks both effective optimization techniques and any sort of benchmark for effectiveness.
In this thrust, we propose to address both missing pieces.

\subsubsection{Removing Unnecessary Identity Checks}

As mentioned above, choreographic programs are compiled through an operation known as \emph{endpoint projection~(EPP)}, which extracts a program for each participant in the system.
To implement process polymorphism, EPP relies on the principle of \emph{autognosis}: every participant knows their own identity.
This is formalized in the network language via a primitive $\AmIN$ operation~\citep{GraversenHM24},
which branches based on the relationship between the current party and a given value.
For instance, $\AmI{\ell}{C_1}{C_2}$ means ``if the party running this program has name~$\ell$, run~$C_1$, otherwise run~$C_2$.''
The PIs have generalized this to \AmIinN, which tests if the running program's name is in a given set~\citep{SamuelsonHC25}.

In current choreographic theory, \emph{every} process-polymorphic function starts with as many $\AmIN$/$\AmIinN$ operations as there are process parameters.
This approach can produce an exponential blowup in code size---the projection of a process-polmorphic function with~$k$ process parameters will have $2^k$ copies of the code.
Moreover, these branches have a (small) cost at run time, leading to a linear performance cost.

In the worst case, this cost in both code size and performance appears inherent.
For instance, consider the following choreographic pseudocode:
\newcommand{\RandSets}{\programfont{randomSubsets}}
\[
  \LetIn{(\rho_1, \rho_2, \rho_3)}{\RandSets(\{\Alice, \Bob, \Cathy, \Dave\})}{(\rho_1.1, \rho_2.2, \rho_3.3)}
\]
This program randomly splits the set of participants $\{\Alice, \Bob, \Cathy, \Dave\}$ into three dynamic sets, $\rho_1$,~$\rho_2$, and~$\rho_3$, potentially with overlap.
For each of~$i = 1, 2, 3$, locations in~$\rho_i$ produce the value~$i$.
Since any participant could end up in any~$\rho_i$ independently, and which set(s) they are in is determined at run time, everyone must have eight code paths:
one for each combination of being in~$\rho_1$, $\rho_2$, and/or~$\rho_3$.
EPP thus compiles the body of the \LetN to a three-deep sequence of nested \AmIinN statements,
each checking if the current party is in a different~$\rho_i$.

While this example shows that the current (na\"ive) solution is optimal in some cases, in common cases it is not.
Process polymorphism is often used to only select between a few possible inputs.
We can already see this in the above example.
A process $\LocFont{E}$ that is not any of~\Alice, \Bob, \Cathy, or~\Dave does \emph{not} need multiple code paths,
as we statically know it cannot be in any~$\rho_i$.
Similarly, if we replaced \RandSets with a partitioning function---ensuring~$\rho_1$, $\rho_2$, and~$\rho_3$ are disjoint---%
\Alice,~\Bob, \Cathy, and~\Dave would each only need three code paths.

Using existing binary \AmIN statements creates a nested branching structure that is challenging to analyze.
We therefore propose to flatten these trees using a new multi-way \AmIN
analogous to \programfont{switch}--\programfont{case} flattening multi-branch \programfont{if} statement.
%Even when we do need an \AmIN instruction, however, we may be able to do better by allowing multi-way \AmIN expressions analogous to \programfont{switch} statements vis-a-vis \programfont{if} statements.
For instance, the body of the \LetN in the above example could project to the following.
\[
  \AmIinN \begin{array}[t]{@{}l@{}}
    {}\mid \rho_1 \cap \rho_2 \cap \rho_3 \Rightarrow (1, 2, 3) \\
    {}\mid \rho_1 \cap \rho_2 \Rightarrow (1, 2) \\
    \quad \vdots \\
    {}\mid \rho_2 \Rightarrow 2 \\
    {}\mid \rho_3 \Rightarrow 3 \\
    {}\mid \_ \Rightarrow () \\
  \end{array}
\]
This transformation converts a nested branching structure into a succinct dispatch table.
The \AmIinN should dispatch to the instructions of the first set that contains the current party's name.

This perspective enables substantial space for optimization.
First, the extensive literature on compilation of object-oriented languages contains many optimizations for dispatch tables,
and we expect some of them to be applicable.
For instance, we will investigate how different representations of identities and location sets
might allow for dispatch with constant-time address lookups rather than linear-time inclusion checks.

Second, while multi-way \AmIN does not reduce code expansion in this intentionally-contrived worse-case example,
it makes it considerably easier to identify when entire code paths are impossible and eliminate them statically.
If we replace \RandSets by a partitioning function, as suggested above,
the four clauses intersecting two or more partitions must have empty location sets.
Since no location is in~$\varnothing$, those options can be removed entirely,
leaving only four branches for~\Alice, \Bob, \Cathy, and~\Dave, and the constant~$()$ for everyone else.

To identify situations where constant-time address lookups are feasible and entire code paths are impossible,
we propose to employ static analysis to determine which participants might be in a dynamic set.
In the example above, \RandSets is given the constant set $\{\Alice, \Bob, \Cathy, \Dave\}$ as an argument,
so those are the only locations that can be in~$\rho_1$, $\rho_2$, and~$\rho_3$.
Note that tracking this information through polymorphic function calls becomes considerably more complicated.

We will investigate multiple static analysis techniques in the literature that appear suited to provide the information needed for this optimization.
In particular, abstract interpretation~\citep{CousotC77}, type-and-effect systems~\citep{NielsonN99}
and refinement types~\citep{FreemanP91} can all track which locations might be in a given set.
We plan to investigate each as possibilities.
By providing EPP with information about which locations might be in each set,
we can optimize the projection to only produce necessary \AmIN statements,
avoiding both code blowup and unnecessary performance costs.

\paragraph{Redundant Identity Checks}
Another significant gain can come from removing redundant identity checks.
A common pattern where they arise is recursive process-polymorphic functions that use the same process at every recursive call.
For instance, consider the following function that takes a tree with data at location variable~$\alpha$
and returns a tree with the same data at~$\beta$, sending the data one piece at a time.
\[
  \newcommand{\TreeSend}{\programfont{tree\_send}}
  \Fun{\TreeSend}{\alpha, \beta, T}{%
    \begin{array}[t]{@{}l@{}}
      \ChorFont{match}~T~\ChorFont{with} \\
      {}\mid \programfont{Leaf} \Rightarrow \programfont{Leaf} \\
      {}\mid \programfont{Node}(L, \alpha.x, R) \Rightarrow \programfont{Node}(
      \begin{array}[t]{@{}l@{}}
        \TreeSend(\alpha, \beta, L), \\
        \alpha.x \ColSend \beta, \\
        \TreeSend(\alpha, \beta, R))
      \end{array}
    \end{array}
  }
\]
Note that every recursive call to \programfont{tree\_send} has the same process parameters as the original call.
Existing EPP definitions simply insert \AmIN instructions at the beginning of the recursive function,
meaning the relevant process must verify its identity on every recursive call.
However, the participants never change.
The first \AmIN check is necessary, but every subsequent check will produce the same result.

We propose to eliminate these sorts of redundancies.
For recursive polymorphic functions, we can compile to two different non-polymorphic recursive functions,
one where the location variable resolves to the current party and one where it does not.
The full function then uses a single \AmIN to select which non-polymorphic function to execute.
This optimization removes a run-time identity check from every recursive invocation of the function,
eliminating an unnecessary performance cost linear in the recursion depth.

While it may appear that using \AmIN to branch between two recursive functions will
lead to the same exponential code blowup as existing na\"ive uses of \AmIN,
it is simply shifting the code expansion from inside the polymorphic function to outside.
Moreover, the static analysis proposed above is, again, precisely the solution to mitigate any concern.

These ideas lead to the following goal for optimizing compilation of process polymorphism:
\begin{goal}
  \label{goal:opt-epp}
  Create an optimized version of endpoint projection for process polymorphism
  that produces smaller program sizes and better run-time performance in common cases
  using multi-way \AmIN statements and elimination of redundant identity checks.
\end{goal}

For each of the optimizations mentioned, we will prove that the optimization is semantics-preserving.
That is, an optimized program must behave identically to its unoptimized counterpart, except for performance.
This strong result ensures that any reasoning and analysis based on the less-performant
existing compilation structures---including, critically, deadlock freedom guarantees---will remain valid in the optimized systems.

\subsubsection{Benchmarking Compilation Performance}

To measure the effectiveness of our proposed optimizations, we need an appropriate set of benchmarks.
However, process-polymorphic choreographies were only introduced in 2024~\citep{GraversenHM24}
and work on them has been entirely theoretical, lacking any publicly-available concrete compiler implementations.
As a result, there do not yet exist any benchmark suites for EPP performance.

We propose to build such a suite for process polymorphic programs to measure the impact of compiler optimizations.
While we will use this new suite to assess, guide, and validate the structure of our own optimizations,
we will design it as a stand-alone benchmark suite that others can also use to measure process polymorphic choreographic performance in the future.

In building the suite, we will include best-case and worst-case benchmarks, but the bulk of the suite should be ``representative'' examples of process-polymorphic code.
The best-case and worst-case benchmarks will be similar to the example described above
with partitioned sets and independently random ones, respectively.
Developing representative examples is more challenging due to the newness of process-polymorphic choreographies.
We therefore propose to create a new repository of process-polymorphic code to discover how it is used in practice.
We will leverage recent growth in choreographic implementations~\citep{ShenKK23,BatesK+25}
to help assemble this corpus, as well as building our own case-studies using these implementations and our own.

\begin{goal}
  \label{goal:benchmark}
  Develop a representative benchmark suite for process-polymorphic EPP with general applicability.
\end{goal}

%For each of these three optimizations, we plan to prove that the resulting version of EPP is deadlock free.
%The code-size and performance savings, however, will usually be common-case, rather than worst case.
%We therefore plan to validate these metrics empirically.
%In order to do so, we need a benchmark for process-polymorphic EPP and an implementation of our optimizations.
%
%\begin{goal}
%  Develop a benchmark suite for process-polymorphic EPP.
%\end{goal}
%
%We plan to develop a benchmark of programs that make use of process polymorphism in order to validate choreographic compilers.
%In particular, we plan to include in our benchmarks the worst-case example sketched above as well as programs designed to be amenable to optimization.
%We would like the bulk of the benchmark to be ``representative'' examples of process-polymorphic code.
%Since process polymorphism is so new, this would require creating a large repository of process-polymorphic code in order to discover how it is used in practice.
%
%\begin{goal}
%  Implement an optimizing version of process-polymorphic EPP.
%  Use it to experimentally validate our optimizations.
%\end{goal}

\paragraph{Preliminary Work}
PI~Hirsch has preliminary work in the form of a compiler for a functional choreographic programming language.
This compiler has been built in no small part by a team of undergraduate students participating in SUNY~Buffalo's \emph{Experiential Learning and Research~(ELR)} program.
This program allows students to complete a significant portion of their degree by participating in small teams under a professor's (or industrial ``client's'') direction on a long-term project for three semesters.
PI~Hirsch's team has recently graduated its first group of students, and currently has students on their first and second semesters.
They design and develop the compiler under the direction of PI~Hirsch and his team of graduate students.

As part of the proposed work, we will enhance the compiler with support for process polymorphism in both the optimized and unoptimized form.
This will be performed by a combination of graduate and undergraduate students.
Moreover, the undergraduate students will implement the benchmark suite of process-polymorphic programs described above.
This will not only allow us to experimentally verify our optimizations, but also (further) train undergraduate students in compiler design, functional programming, and experimental practice.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "desc"
%%% End:
